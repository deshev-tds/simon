import torch
import sounddevice as sd
import numpy as np
import time
from openai import OpenAI
from faster_whisper import WhisperModel

# --- CONFIG ---
SAMPLE_RATE = 16000
VAD_THRESHOLD = 0.5     # –ù–∞–¥ —Ç–æ–≤–∞ –µ –≥–æ–≤–æ—Ä
SILENCE_DURATION = 1.0  # –ö–æ–ª–∫–æ —Å–µ–∫—É–Ω–¥–∏ —Ç–∏—à–∏–Ω–∞, –∑–∞ –¥–∞ –ø—Ä–∏–∫–ª—é—á–∏–º –∑–∞–ø–∏—Å–∞
MIN_RECORDING = 0.5     # –î–∞ –Ω–µ –ø—Ä–∞—â–∞–º–µ –ø—Ä–∞–∑–Ω–∏ —à—É–º–æ–≤–µ
CHUNKS_PER_SEC = 32     # –ü—Ä–∏ 512 samples/chunk (32ms)

# --- SETUP ---
# 1. Load VAD (—Å—É–ø–µ—Ä –ª–µ–∫ –µ)
model_vad, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',
                                  model='silero_vad',
                                  force_reload=False,
                                  onnx=True) # M4 Pro –æ–±–∏—á–∞ ONNX
(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
vad_iterator = VADIterator(model_vad)

# 2. Clients
client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")
stt_model = WhisperModel("medium.en", device="cpu", compute_type="int8")

# --- STATE ---
is_ai_speaking = False

def speak_response(text):
    global is_ai_speaking
    is_ai_speaking = True
    print(f"ü§ñ AI: {text}")
    
    # 1. –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –∞—É–¥–∏–æ (–¢—É–∫ –≤–∏–∫–∞—à Kokoro/TTS)
    # audio = generate_tts(text)
    
    # 2. Playback (Blocking - —á–∞–∫–∞–º–µ –¥–∞ —Å–≤—ä—Ä—à–∏!)
    # sd.play(audio, 24000)
    # sd.wait() 
    
    # –°–∏–º—É–ª–∞—Ü–∏—è –∑–∞ —Ç–µ—Å—Ç–∞:
    time.sleep(len(text) * 0.05) 
    
    print("‚úÖ AI done speaking.")
    is_ai_speaking = False
    # –†–µ—Å–µ—Ç–≤–∞–º–µ VAD-–∞, –∑–∞ –¥–∞ –Ω–µ "—á—É–µ" –µ—Ö–æ—Ç–æ –∫–∞—Ç–æ –Ω–æ–≤–∞ —Ä–µ—á –≤–µ–¥–Ω–∞–≥–∞
    vad_iterator.reset_states()

def main_loop():
    print("üé§ Mic Listening... (Silence threshold: 1.0s)")
    
    buffer = []
    silence_chunks = 0
    is_recording_speech = False
    
    # Callback –∑–∞ –∞—É–¥–∏–æ —Å—Ç—Ä–∏–º–∞
    def callback(indata, frames, time, status):
        nonlocal silence_chunks, is_recording_speech, buffer
        
        # 1. –ì–õ–£–• –†–ï–ñ–ò–ú: –ê–∫–æ AI –≥–æ–≤–æ—Ä–∏, –∏–≥–Ω–æ—Ä–∏—Ä–∞–º–µ –≤—Ö–æ–¥–∞
        if is_ai_speaking:
            return

        # Convert to float32 for VAD
        audio_chunk = indata.flatten()
        
        # 2. VAD Check
        speech_prob = model_vad(torch.from_numpy(audio_chunk), SAMPLE_RATE).item()
        
        if speech_prob > VAD_THRESHOLD:
            is_recording_speech = True
            silence_chunks = 0
            buffer.extend(audio_chunk)
        elif is_recording_speech:
            # –í–µ—á–µ —Å–º–µ –ø–æ—á–Ω–∞–ª–∏ –¥–∞ –∑–∞–ø–∏—Å–≤–∞–º–µ, –Ω–æ —Å–µ–≥–∞ –µ —Ç–∏—Ö–æ
            silence_chunks += 1
            buffer.extend(audio_chunk)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–ª–∏ —Ç–∏—à–∏–Ω–∞—Ç–∞ –µ –¥–æ—Å—Ç–∞—Ç—ä—á–Ω–æ –¥—ä–ª–≥–∞ (1 —Å–µ–∫)
            chunks_needed = int(SILENCE_DURATION * (SAMPLE_RATE / 512))
            if silence_chunks > chunks_needed:
                # –ö–†–ê–ô –ù–ê –¢–£–†–ù-–∞
                process_turn(np.array(buffer))
                # –†–µ—Å–µ—Ç
                buffer = []
                is_recording_speech = False
                silence_chunks = 0

    # Start Stream
    with sd.InputStream(callback=callback, channels=1, samplerate=SAMPLE_RATE, blocksize=512):
        while True:
            sd.sleep(100) # –î—ä—Ä–∂–∏ —Å–∫—Ä–∏–ø—Ç–∞ –∂–∏–≤

def process_turn(audio_data):
    if len(audio_data) / SAMPLE_RATE < MIN_RECORDING:
        return # –¢–≤—ä—Ä–¥–µ –∫—Ä–∞—Ç–∫–æ, —Å–∏–≥—É—Ä–Ω–æ –µ —à—É–º
        
    print("Processing user audio...")
    
    # 1. Whisper Transcribe
    segments, _ = stt_model.transcribe(audio_data, beam_size=5)
    user_text = " ".join([s.text for s in segments]).strip()
    
    if not user_text: return
    print(f"üë§ User: {user_text}")

    # 2. Check for Trigger Word (Optional refinement)
    # if "jarvis" not in user_text.lower(): return

    # 3. LLM Request
    stream = client.chat.completions.create(
        model="local-model",
        messages=[{"role": "user", "content": user_text}],
        stream=True
    )

    # 4. Stream & Speak
    # –¢—É–∫ —Å—ä–±–∏—Ä–∞—à –∏–∑—Ä–µ—á–µ–Ω–∏—è –∏ –≥–∏ –ø—Ä–∞—â–∞—à –Ω–∞ speak_response()
    full_response = ""
    for chunk in stream:
        if chunk.choices[0].delta.content:
            full_response += chunk.choices[0].delta.content
    
    speak_response(full_response)

if __name__ == "__main__":
    main_loop()